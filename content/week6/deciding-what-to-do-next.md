---
title: "تصمیم گیری درباره اقدام بعدی"
date: 2020-10-17T20:45:00+03:30
draft: false
weight: 60
---

روند تصمیم‌گیری ما می‌تواند به مراحل زیر تقسیم‌بندی شود:

- جمع‌آوری نمونه آموزش بیشتر\
اصلاح واریانس زیاد
- استفاده از مجموعه کوچکتری از ویژگی‌ها\
اصلاح واریانس زیاد
- اضافه کردن ویژگی\
اصلاح بایاس زیاد
- اضافه کردن ویژگی‌های چندجمله‌ای\
اصلاح بایاس زیاد
- کاهش  $\lambda$\
اصلاح بایاس زیاد
- افزایش  $\lambda$\
اصلاح واریانس زیاد


**تشخیص شبکه‌های عصبی**

- یک شبکه عصبی با تعداد پارامترهای کم مستعد underfitting خواهد بود. همچنین این شبکه عصبی **محاسبات ارزانی** خواهد داشت.

- یک شبکه عصبی با تعداد بیشتری از پارامتر مستعد overfitting خواهد بود و **محاسبات سنگینی** خواهد داشت. در این شرایط می‌توانید از منظم‌سازی (افزایش $\lambda$) برای اصلاح overfitting استفاده کنید.

استفاده از یک لایه پنهان واحد پیش‌ فرض مناسبی برای شروع است. با کمک مجموعه cross validation می‌توان شبکه عصبی را روی تعدادی از لایه‌های پنهان آموزش داد.

**انتخاب مدل:**

انتخاب ترتیب چندجمله‌ای‌ها(M).\
چگونه می‌توانیم تعیین کنیم که کدام یک از پارامترهای $\theta$ را در مدل قرار دهیم(معروف به "انتخاب مدل")؟

راه‌های گوناگونی برای حل این مشکل وجود دارد:
- جمع‌آوری دیتای بیشتر(که روش بسیار سختی است).
- انتخاب مدلی که بیشترین سازگاری را با دیتا دارد و باعث overfitting نخواهد شد (این روش هم بسیار مشکل است).
- کم کردن امکان overfitting از طریق منظم‌سازی.

**بایاس: خطای تخمینی (اختلاف مقدار مورد نظر و مقدار بهینه)**

- بایاس زیاد = Underfitting(BU)
- $J_{train }\left ( \Theta  \right )$ و $J_{CV }\left ( \Theta  \right )$ مقادیر بالایی خواهند داشت و $J_{train }\left ( \Theta  \right ) \approx J_{CV}\left ( \Theta  \right )$

**واریانس: خطای تخمین به دلیل داده‌های محدود**

- واریانس زیاد = Overfitting(VO)
- $J_{train }\left ( \Theta  \right )$ کم خواهد بود و $J_{CV }\left ( \Theta  \right ) \gg  J_{train}\left ( \Theta  \right )$

**آگاهی برای متوازن کردن بایاس و واریانس**

- مدل پیچیده => حساس به دیتا => تحت تاثیر تغییرات در X => واریانس زیاد، بایاس کم
- مدل ساده => پایدارتر => با تغییرات X چندان تغییر نمی‌کند => واریانس کم، بایاس زیاد

یکی از مهم‌ترین اهداف در یادگیری: پیدا کردن صحیح‌ترین مدل در متوازن کردن بایاس و واریانس

**تاثیرات منظم‌سازی:**

- مقادیر کوچک $\lambda$ به مدل اجازه می‌دهند تا نسبت به اختلالاتی که به واریانس بزرگ منتهی می‌شوند، به خوبی وفق پیدا کنند => Overfitting.

- مقادیر بزرگ $\lambda$ پارامترهای وزن را که به بایاس بزرگ منتهی می‌شوند به صفر می‌رساند => Underfitting.