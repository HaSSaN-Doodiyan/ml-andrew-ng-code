---
title: "تصمیم گیری درباره اقدام بعدی"
date: 2020-10-17T20:45:00+03:30
draft: false
weight: 60
---

روند تصمیم‌گیری ما می‌تواند به به شرح زیر باشد:

- جمع‌آوری نمونه آموزشی بیشتر:
اصلاح واریانس زیاد
- استفاده از مجموعه کوچکتری از ویژگی‌ها:
اصلاح واریانس زیاد
- اضافه کردن ویژگی:
اصلاح بایاس زیاد
- اضافه کردن ویژگی‌های چندجمله‌ای:
اصلاح بایاس زیاد
- کاهش  $\lambda$:
اصلاح بایاس زیاد
- افزایش  $\lambda$:
اصلاح واریانس زیاد


### تشخیص شبکه‌های عصبی

- یک شبکه عصبی با تعداد پارامترهای کم مستعد underfitting خواهد بود. همچنین این شبکه عصبی از نظر محاسباتی **ارزان** است.

- یک شبکه عصبی با تعداد بیشتری از پارامتر مستعد overfitting خواهد بود و از نظر محاسباتی **گران** است. در این شرایط می‌توانید از منظم‌سازی (افزایش $\lambda$) برای اصلاح overfitting استفاده کنید.

استفاده از یک عدد لایه پنهان پیش‌ فرض مناسبی برای شروع است. با کمک مجموعه cross validation می‌توان شبکه عصبی را روی تعدادی از لایه‌های پنهان آموزش داد.

### انتخاب مدل

انتخاب M به ترتیب چندجمله‌ای‌ها.\
چگونه می‌توانیم تعیین کنیم که کدام یک از پارامترهای $\theta$ را در مدل قرار دهیم (معروف به "انتخاب مدل")؟

راه‌های گوناگونی برای حل این مسئله وجود دارد:
- جمع‌آوری داده بیشتر (که روش بسیار سختی است).
- انتخاب مدلی که بیشترین سازگاری را با داده دارد و باعث overfitting نخواهد شد (این روش هم بسیار مشکل است).
- کم کردن امکان overfitting از طریق منظم‌سازی.

**بایاس: خطای تقریب (اختلاف مقدار مورد نظر و مقدار بهینه)**

- بایاس زیاد = Underfitting (BU)
- $J_{train }\left ( \Theta  \right )$ و $J_{CV }\left ( \Theta  \right )$ مقادیر زیادی خواهند داشت و $J_{train }\left ( \Theta  \right ) \approx J_{CV}\left ( \Theta  \right )$

**واریانس: خطای تخمین به دلیل داده‌های محدود**

- واریانس زیاد = Overfitting (VO)
- $J_{train }\left ( \Theta  \right )$ کم خواهد بود و $J_{CV }\left ( \Theta  \right ) \gg  J_{train}\left ( \Theta  \right )$

**آگاهی برای متوازن کردن بایاس و واریانس**

- مدل پیچیده $\Leftarrow$ حساس به دیتا $\Leftarrow$ تحت تاثیر تغییرات در X $\Leftarrow$ واریانس زیاد، بایاس کم
- مدل ساده $\Leftarrow$ پایدارتر $\Leftarrow$ با تغییرات X چندان تغییر نمی‌کند $\Leftarrow$ واریانس کم، بایاس زیاد

یکی از مهم‌ترین اهداف در یادگیری: پیدا کردن مدلی که در بایاس و واریانس توازن داشته باشد.

**تاثیرات منظم‌سازی:**

- مقادیر کوچک $\lambda$ به مدل اجازه می‌دهند تا نسبت به اختلالاتی که به واریانس بزرگ منتهی می‌شوند، به خوبی وفق پیدا کنند $\Leftarrow$ Overfitting.

- مقادیر بزرگ $\lambda$ پارامترهای وزن را که به بایاس بزرگ منتهی می‌شوند به صفر می‌رساند $\Leftarrow$ Underfitting.

**تاثیرات پیچیدگی مدل:**

- چندجمله‌ای‌های درجه پایین (پیچیدگی پایین مدل) بایاس زیاد و واریانس کم دارند. در این شرایط مدل همواره سازگاری کمی خواهد داشت.

- چندجمله‌ای‌های درجه بالا (پیچیدگی بالای مدل) با داده آموزشی سازگاری بسیار خوب و با داده آزمون سازگاری بسیار کمی خواهد داشت. این مسئله باعث بایاس کم و واریانس بسیار زیاد روی داده آموزشی خواهد شد.

-  در واقعیت، ما می‌خواهیم مدل میانه‌ای را انتخاب کنیم که به خوبی اعتبارسنجی شود و همچنین به خوبی با داده سازگار باشد.

**یک قانون معمول هنگام اجرای تشخیص:**

- نمونه‌های آموزشی بیشتر واریانس زیاد را اصلاح می‌کند اما تاثیری بر بایاس زیاد ندارد.

- ویژگی‌های کمتر واریانس زیاد را اصلاح می‌کند اما تاثیری بر بایاس زیاد ندارد.

- ویژگی‌های اضافه بایاس زیاد را اصلاح می‌کند اما بر واریانس زیاد تاثیری ندارد.

- اضافه کردن چند‌جمله‌ای  و ویژگی‌های متقابل بایاس زیاد را اصلاح می‌کند اما تاثیری بر واریانس زیاد ندارد.

- هنگام استفاده از گرادیان کاهشی، کاهش $\lambda$ می‌تواند بایاس زیاد را اصلاح کند و افزایش $\lambda$ واریانس زیاد را اصلاح می‌کند. ($\lambda$ پارامتر منظم‌سازی است.) 

- هنگام استفاده از شبکه‌های عصبی، شبکه‌های کوچک بیشتر در معرض Underfitting و شبکه‌های بزرگ بیشتر در معرض Overfitting هستند.
