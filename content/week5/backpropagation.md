---
title: "Backpropagation"
date: 2020-10-01T12:30:31+03:30
draft: false
weight: 20
---

اصطلاح **Backpropagation** یا به فارسی **پس انتشار**
در شبکه های عصبی، برای به حداقل رساندن تابع هزینه (minimizing cost function)
مثل کاری که با گرادیان کاهشی در رگرسیون لجستیک و خطی انجام می‌دادیم، استفاده می‌شود.

هدف ما محاسبه این است:

$$
min_\Theta J(\Theta)
$$

یعنی می‌خواهیم تابع هزیه $J$ را با استفاده از یک محموعه
بهینه از پارامتر $\Theta$ به حداقل برسانیم
(یا به عبارتی دیگر مینیمم کنیم).

در این بخش به معادلاتی که برای محاسبه مشتق جزئی تابع $J(\Theta)$ استفاده می‌کنیم، خواهیم پرداخت:

$$
\frac{\partial}{\partial \Theta_ {i,j} ^{(l)}} J(\Theta)
$$

در Backpropagation ما برای هر گره (node) محاسبه خواهیم کرد:

$
= \delta_j ^{(l)}
$
خطا هر گره $j$ ام در لایه $l$

{{% notice note %}}
به خاطر آورید که 
$ \delta_j ^{(l)} $
گره فعال ساز $j$ در لایه $l$ است.
{{% /notice %}}

برای آخرین لایه یا همان لایه خروجی، می‌توانیم وکتور مقادیر دلتا را به این صورت حساب کنیم:
$$
\delta ^{(L)} = a ^{(L)} - y
$$

به طوری که $L$ تعداد کل لایه های ما است و
$a ^{(L)}$ وکتور خروجی گره های فعال ساز در آخرین لایه است.
بنابراین مقادیر خطای ما برای آخرین لایه، تفاوت نتایج واقعی در لایه آخر و خروجی صحیح در 
$y$ است.


برای محاسبه مقادیر دلتای، لایه های قبل از لایه آخر می‌توانیم از معادله ای استفاده کنیم که ما را از سمت راست به چپ عقب می‌‌برد:

$$
\delta ^{(l)} = ( (\Theta ^ {(l)}) ^ T  \delta ^ {(l+1)}) \hspace{0.2cm} .*  \hspace{0.2cm} {g}' (z^{(l)})
$$

مقادیر دلتای لایه $l$ با ضرب مقادیر دلتا در لایه بعدی با ماتریس تتا لایه $l$ محاسبه می‌شود،
سپس به صورت element-wise آن را در تابعی به اسم g-prime یا همان $g'$ ضرب می‌کنیم.

که $g'$ مشتق تابع فعال سازی $g$ است، و به عنوان ورودی $z^{(l)}$ را می‌گیرد.

بخش $g'$ را به صورت زیر نیز می‌توان نوشت:
$$
g'(u) = g(u) \hspace{0.2cm} .*  \hspace{0.2cm} (1 - g(u))
$$

و معادله کامل B‌ackpropagation برای گره های داخلی را به این شکل می‌نویسیم:
$$
\delta ^{(l)} = ( (\Theta ^ {(l)}) ^ T  \delta ^ {(l+1)}) \hspace{0.2cm} .*  \hspace{0.2cm} a^{(l)}
\hspace{0.2cm} .*  \hspace{0.2cm}  (1 -  a^{(l)})
$$